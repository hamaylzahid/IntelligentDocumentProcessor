{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663
    },
    "id": "R-aTT1byrW8e",
    "outputId": "c23d7141-1245-4176-f0bf-be3e32d3eedd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:easyocr.easyocr:Using CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
      "* Running on public URL: https://4502a33b04740420b4.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://4502a33b04740420b4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Intelligent Document Image Processing System (Clean & Structured)\n",
    "# ============================================================\n",
    "\n",
    "# -------------------------------\n",
    "# Imports & Setup\n",
    "# -------------------------------\n",
    "import os, re, json\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import pytesseract\n",
    "import easyocr\n",
    "import camelot\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "import gradio as gr\n",
    "\n",
    "# -------------------------------\n",
    "# OCR Setup\n",
    "# -------------------------------\n",
    "pytesseract.pytesseract.tesseract_cmd = \"/usr/bin/tesseract\"\n",
    "ocr_reader = easyocr.Reader(['en'], gpu=False)\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "# -------------------------------\n",
    "# Helper Functions\n",
    "# -------------------------------\n",
    "def convert_pptx_to_pdf(pptx_path):\n",
    "    os.system(f'libreoffice --headless --convert-to pdf \"{pptx_path}\" --outdir .')\n",
    "    return pptx_path.replace(\".pptx\", \".pdf\")\n",
    "\n",
    "def load_images(file_path):\n",
    "    images = []\n",
    "    if file_path.lower().endswith(\".pptx\"):\n",
    "        file_path = convert_pptx_to_pdf(file_path)\n",
    "    if file_path.lower().endswith(\".pdf\"):\n",
    "        pages = convert_from_path(file_path, dpi=200)\n",
    "        for i, page in enumerate(pages):\n",
    "            img_path = f\"page_{i}.png\"\n",
    "            page.save(img_path, \"PNG\")\n",
    "            images.append(img_path)\n",
    "    elif file_path.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "        images.append(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format\")\n",
    "    return images\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.GaussianBlur(gray, (5,5), 0)\n",
    "    thresh = cv2.adaptiveThreshold(blur, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                   cv2.THRESH_BINARY, 31, 2)\n",
    "    return thresh\n",
    "\n",
    "# -------------------------------\n",
    "# OCR Functions\n",
    "# -------------------------------\n",
    "def extract_easyocr_text(image_path):\n",
    "    lines = ocr_reader.readtext(image_path, detail=0)\n",
    "    lines = [l for l in lines if len(l) > 2 and not re.match(r\"^[^a-zA-Z0-9]+$\", l)]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def extract_tesseract_text(image):\n",
    "    config = \"--oem 1 --psm 6\"\n",
    "    return pytesseract.image_to_string(image, config=config)\n",
    "\n",
    "def extract_handwritten_text(image):\n",
    "    config = \"--oem 1 --psm 6\"\n",
    "    return pytesseract.image_to_string(image, config=config)\n",
    "\n",
    "# -------------------------------\n",
    "# Layout / Structure Helpers\n",
    "# -------------------------------\n",
    "def split_into_paragraphs(text):\n",
    "    return [p.strip() for p in text.split(\"\\n\\n\") if len(p.strip()) > 10]\n",
    "\n",
    "def detect_headings(text):\n",
    "    return [line.strip() for line in text.split(\"\\n\") if len(line) > 3 and line.isupper()]\n",
    "\n",
    "def detect_multicolumn_text(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    h, w = img.shape[:2]\n",
    "    left = img[:, :w//2]\n",
    "    right = img[:, w//2:]\n",
    "    return [extract_tesseract_text(left), extract_tesseract_text(right)]\n",
    "\n",
    "# -------------------------------\n",
    "# Cleaning Functions\n",
    "# -------------------------------\n",
    "def clean_headings(headings):\n",
    "    cleaned = []\n",
    "    for h in headings:\n",
    "        letters = sum(c.isalpha() for c in h)\n",
    "        if letters > 2:\n",
    "            cleaned.append(h)\n",
    "    return cleaned\n",
    "\n",
    "def clean_paragraphs(paragraphs):\n",
    "    seen = set()\n",
    "    cleaned = []\n",
    "    for p in paragraphs:\n",
    "        p_clean = re.sub(r'[^a-zA-Z0-9\\s,.]', '', p)\n",
    "        p_clean = re.sub(r'\\s+', ' ', p_clean).strip()\n",
    "        if len(p_clean) > 10 and p_clean not in seen:\n",
    "            cleaned.append(p_clean)\n",
    "            seen.add(p_clean)\n",
    "    return cleaned\n",
    "\n",
    "def clean_columns(columns, paragraphs):\n",
    "    cleaned_columns = []\n",
    "    para_set = set(paragraphs)\n",
    "    for col in columns:\n",
    "        col_clean = []\n",
    "        for line in col.split('\\n'):\n",
    "            line_clean = re.sub(r'[^a-zA-Z0-9\\s,.]', '', line).strip()\n",
    "            if len(line_clean) > 5 and line_clean not in para_set:\n",
    "                col_clean.append(line_clean)\n",
    "        cleaned_columns.append(\"\\n\".join(col_clean))\n",
    "    return cleaned_columns\n",
    "\n",
    "# -------------------------------\n",
    "# Document / Table / Form Extraction\n",
    "# -------------------------------\n",
    "def detect_document_type(text):\n",
    "    t = text.lower()\n",
    "    if \"certificate\" in t or \"has successfully completed\" in t:\n",
    "        return \"certificate\"\n",
    "    if \":\" in t and \"date\" in t:\n",
    "        return \"form\"\n",
    "    return \"generic\"\n",
    "\n",
    "def extract_key_values(text):\n",
    "    kv = {}\n",
    "    for line in text.split(\"\\n\"):\n",
    "        if \":\" in line:\n",
    "            k,v = line.split(\":\",1)\n",
    "            if 2 < len(k) < 40 and 1 < len(v) < 100:\n",
    "                kv[k.strip()] = v.strip()\n",
    "    return kv\n",
    "\n",
    "def extract_tables_from_pdf(pdf_path):\n",
    "    if not pdf_path.lower().endswith(\".pdf\"): return []\n",
    "    tables = camelot.read_pdf(pdf_path, pages=\"all\", flavor=\"stream\")\n",
    "    outputs = []\n",
    "    for i, table in enumerate(tables):\n",
    "        csv_path = f\"outputs/table_{i}.csv\"\n",
    "        table.df.to_csv(csv_path, index=False)\n",
    "        outputs.append({\"table_index\":i,\"csv_path\":csv_path,\"data\":table.df.to_dict(orient=\"records\")})\n",
    "    return outputs\n",
    "\n",
    "def extract_contact_info(text):\n",
    "    urls = re.findall(r'\\b(?:https?://)?(?:www\\.)?[\\w\\-]+\\.[\\w\\.\\-]+\\b', text, re.IGNORECASE)\n",
    "    emails = re.findall(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', text)\n",
    "    phones = re.findall(r'\\b\\d{7,15}\\b', text)\n",
    "    categories = re.findall(r'Pet Supplies|Games n Toys', text, re.IGNORECASE)\n",
    "    return {\"urls\": urls, \"emails\": emails, \"phones\": phones, \"categories\": categories}\n",
    "\n",
    "# -------------------------------\n",
    "# Full Pipeline (Gradio)\n",
    "# -------------------------------\n",
    "def process_document(file_obj):\n",
    "    file_path = file_obj.name\n",
    "\n",
    "    if not file_path.lower().endswith((\".pdf\", \".pptx\", \".png\", \".jpg\", \".jpeg\")):\n",
    "        return {\"error\":\"Unsupported file type\"}, pd.DataFrame()\n",
    "\n",
    "    image_paths = load_images(file_path)\n",
    "    results = {\"pages\":[], \"tables\":extract_tables_from_pdf(file_path)}\n",
    "\n",
    "    for img_path in image_paths:\n",
    "        processed_img = preprocess_image(img_path)\n",
    "        easy_text = extract_easyocr_text(img_path)\n",
    "        tess_text = extract_tesseract_text(processed_img)\n",
    "        handwritten_text = extract_handwritten_text(processed_img)\n",
    "        combined_text = \"\\n\".join([easy_text, tess_text, handwritten_text]).strip()\n",
    "        multicol_texts = detect_multicolumn_text(img_path)\n",
    "\n",
    "        # Clean & structured\n",
    "        paragraphs = clean_paragraphs(split_into_paragraphs(combined_text))\n",
    "        headings = clean_headings(detect_headings(combined_text))\n",
    "        columns = clean_columns(multicol_texts, paragraphs)\n",
    "        doc_type = detect_document_type(combined_text)\n",
    "        key_values = extract_key_values(combined_text) if doc_type==\"form\" else {}\n",
    "        contact_info = extract_contact_info(combined_text)\n",
    "\n",
    "        results[\"pages\"].append({\n",
    "            \"document_type\": doc_type,\n",
    "            \"headings\": headings,\n",
    "            \"paragraphs\": paragraphs,\n",
    "            \"columns\": columns,\n",
    "            \"key_value_pairs\": key_values,\n",
    "            \"contacts\": contact_info,\n",
    "            \"text\": combined_text\n",
    "        })\n",
    "\n",
    "    # Save JSON\n",
    "    json_path = \"outputs/output.json\"\n",
    "    with open(json_path,\"w\",encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "    # Contacts table for Gradio\n",
    "    contact_rows = []\n",
    "    for page in results[\"pages\"]:\n",
    "        c = page[\"contacts\"]\n",
    "        for u in c[\"urls\"] or [None]:\n",
    "            for e in c[\"emails\"] or [None]:\n",
    "                for p in c[\"phones\"] or [None]:\n",
    "                    for cat in c[\"categories\"] or [None]:\n",
    "                        contact_rows.append({\"URL\":u,\"Email\":e,\"Phone\":p,\"Category\":cat})\n",
    "    contact_df = pd.DataFrame(contact_rows)\n",
    "    return results[\"pages\"], contact_df\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
